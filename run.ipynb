{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: llama_main.py [-h] [--model MODEL] [--dataset {wikitext2,ptb,c4}]\n",
      "                     [--seed SEED] [--nsamples NSAMPLES] [--percdamp PERCDAMP]\n",
      "                     [--sparsity SPARSITY] [--prunen PRUNEN] [--prunem PRUNEM]\n",
      "                     [--blocksize BLOCKSIZE] [--gmp] [--wbits WBITS]\n",
      "                     [--minlayer MINLAYER] [--maxlayer MAXLAYER]\n",
      "                     [--prune_only PRUNE_ONLY] [--invert] [--save SAVE]\n",
      "                     [--true-sequential] [--log_wandb]\n",
      "llama_main.py: error: unrecognized arguments: --cudan cuda:0\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "%run llama_main.py --sparsity 0.0 --cudan \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77d57e9c6c440cc84792c9d49ad0bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Ready.\n",
      "0 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 1.24\n",
      "error 0.3258904218673706\n",
      "0 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 1.06\n",
      "error 0.6269239783287048\n",
      "0 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 1.05\n",
      "error 0.9810671210289001\n",
      "0 self_attn.o_proj\n",
      "Pruning ...\n",
      "time 1.05\n",
      "error 0.048764199018478394\n",
      "0 mlp.up_proj\n",
      "Pruning ...\n",
      "time 1.32\n",
      "error 50.979248046875\n",
      "0 mlp.gate_proj\n",
      "Pruning ...\n",
      "time 1.05\n",
      "error 51.93450164794922\n",
      "0 mlp.down_proj\n",
      "Pruning ...\n",
      "time 2.98\n",
      "error 0.2759826183319092\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.83 GiB. GPU 1 has a total capacity of 22.18 GiB of which 2.82 GiB is free. Including non-PyTorch memory, this process has 19.36 GiB memory in use. Of the allocated memory 16.67 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/sxy/sparseLLM repo/SparseLLM/llama_main.py:47\u001b[0m\n\u001b[1;32m     44\u001b[0m         model\u001b[38;5;241m.\u001b[39msave_pretrained(args\u001b[38;5;241m.\u001b[39msave)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sxy/sparseLLM repo/SparseLLM/llama_main.py:37\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m dataloader, testloader \u001b[38;5;241m=\u001b[39m get_loaders(args\u001b[38;5;241m.\u001b[39mdataset, nsamples\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnsamples, seed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed, model\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel, seqlen\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mseqlen)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (args\u001b[38;5;241m.\u001b[39msparsity \u001b[38;5;129;01mor\u001b[39;00m args\u001b[38;5;241m.\u001b[39mprunen) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mgmp:\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mllama_sparsellm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudan\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwikitext2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc4\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     40\u001b[0m     dataloader, testloader \u001b[38;5;241m=\u001b[39m get_loaders(dataset, seed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed, model\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel, seqlen\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mseqlen)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sxy/sparseLLM repo/SparseLLM/model_utils.py:520\u001b[0m, in \u001b[0;36mllama_sparsellm\u001b[0;34m(model, dataloader, dev, args)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m weight_matrix_1\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Update the weight matrix of mlp.down_proj\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m pinv \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# Calculate the weight matrix\u001b[39;00m\n\u001b[1;32m    522\u001b[0m weight_matrix_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(Y, pinv)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.83 GiB. GPU 1 has a total capacity of 22.18 GiB of which 2.82 GiB is free. Including non-PyTorch memory, this process has 19.36 GiB memory in use. Of the allocated memory 16.67 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "%run llama_main.py --sparsity 0.2 --cudan \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672a58ec8b5e4d3dba2320705a570842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ...\n",
      "Ready.\n",
      "0 self_attn.k_proj\n",
      "Pruning ...\n",
      "torch.Size([2048, 5120]) torch.Size([5120, 5120])\n",
      "Iter 2/100, Loss=0.146973\n",
      "Iter 10/100, Loss=0.146973\n",
      "Iter 20/100, Loss=0.146973\n",
      "Iter 30/100, Loss=0.146851\n",
      "Iter 40/100, Loss=0.146729\n",
      "Iter 50/100, Loss=0.146729\n",
      "Iter 60/100, Loss=0.146606\n",
      "Iter 70/100, Loss=0.146606\n",
      "Iter 80/100, Loss=0.146484\n",
      "Iter 90/100, Loss=0.146484\n",
      "Iter 100/100, Loss=0.146362\n",
      "Starting iterative correction ...\n",
      "Iterative correction step 1/100, MSE loss: 0.032166\n",
      "Iterative correction step 2/100, MSE loss: 0.029846\n",
      "Iterative correction step 3/100, MSE loss: 0.028061\n",
      "Iterative correction step 4/100, MSE loss: 0.026688\n",
      "Iterative correction step 5/100, MSE loss: 0.025635\n",
      "Iterative correction step 6/100, MSE loss: 0.024796\n",
      "Iterative correction step 7/100, MSE loss: 0.024139\n",
      "Iterative correction step 8/100, MSE loss: 0.023605\n",
      "Iterative correction step 9/100, MSE loss: 0.023178\n",
      "Iterative correction step 10/100, MSE loss: 0.022812\n",
      "Iterative correction step 11/100, MSE loss: 0.022491\n",
      "Iterative correction step 12/100, MSE loss: 0.022232\n",
      "Iterative correction step 13/100, MSE loss: 0.022003\n",
      "Iterative correction step 14/100, MSE loss: 0.021790\n",
      "Iterative correction step 15/100, MSE loss: 0.021591\n",
      "Iterative correction step 16/100, MSE loss: 0.021423\n",
      "Iterative correction step 17/100, MSE loss: 0.021271\n",
      "Iterative correction step 18/100, MSE loss: 0.021118\n",
      "Iterative correction step 19/100, MSE loss: 0.020981\n",
      "Iterative correction step 20/100, MSE loss: 0.020844\n",
      "Iterative correction step 21/100, MSE loss: 0.020721\n",
      "Iterative correction step 22/100, MSE loss: 0.020615\n",
      "Iterative correction step 23/100, MSE loss: 0.020493\n",
      "Iterative correction step 24/100, MSE loss: 0.020386\n",
      "Iterative correction step 25/100, MSE loss: 0.020279\n",
      "Iterative correction step 26/100, MSE loss: 0.020187\n",
      "Iterative correction step 27/100, MSE loss: 0.020081\n",
      "Iterative correction step 28/100, MSE loss: 0.019989\n",
      "Iterative correction step 29/100, MSE loss: 0.019897\n",
      "Iterative correction step 30/100, MSE loss: 0.019821\n",
      "Iterative correction step 31/100, MSE loss: 0.019730\n",
      "Iterative correction step 32/100, MSE loss: 0.019653\n",
      "Iterative correction step 33/100, MSE loss: 0.019577\n",
      "Iterative correction step 34/100, MSE loss: 0.019485\n",
      "Iterative correction step 35/100, MSE loss: 0.019409\n",
      "Iterative correction step 36/100, MSE loss: 0.019348\n",
      "Iterative correction step 37/100, MSE loss: 0.019272\n",
      "Iterative correction step 38/100, MSE loss: 0.019196\n",
      "Iterative correction step 39/100, MSE loss: 0.019135\n",
      "Iterative correction step 40/100, MSE loss: 0.019058\n",
      "Iterative correction step 41/100, MSE loss: 0.018997\n",
      "Iterative correction step 42/100, MSE loss: 0.018936\n",
      "Iterative correction step 43/100, MSE loss: 0.018875\n",
      "Iterative correction step 44/100, MSE loss: 0.018799\n",
      "Iterative correction step 45/100, MSE loss: 0.018738\n",
      "Iterative correction step 46/100, MSE loss: 0.018692\n",
      "Iterative correction step 47/100, MSE loss: 0.018631\n",
      "Iterative correction step 48/100, MSE loss: 0.018570\n",
      "Iterative correction step 49/100, MSE loss: 0.018509\n",
      "Iterative correction step 50/100, MSE loss: 0.018463\n",
      "Iterative correction step 51/100, MSE loss: 0.018402\n",
      "Iterative correction step 52/100, MSE loss: 0.018341\n",
      "Iterative correction step 53/100, MSE loss: 0.018295\n",
      "Iterative correction step 54/100, MSE loss: 0.018250\n",
      "Iterative correction step 55/100, MSE loss: 0.018188\n",
      "Iterative correction step 56/100, MSE loss: 0.018143\n",
      "Iterative correction step 57/100, MSE loss: 0.018097\n",
      "Iterative correction step 58/100, MSE loss: 0.018036\n",
      "Iterative correction step 59/100, MSE loss: 0.017990\n",
      "Iterative correction step 60/100, MSE loss: 0.017944\n",
      "Iterative correction step 61/100, MSE loss: 0.017899\n",
      "Iterative correction step 62/100, MSE loss: 0.017853\n",
      "Iterative correction step 63/100, MSE loss: 0.017807\n",
      "Iterative correction step 64/100, MSE loss: 0.017761\n",
      "Iterative correction step 65/100, MSE loss: 0.017715\n",
      "Iterative correction step 66/100, MSE loss: 0.017685\n",
      "Iterative correction step 67/100, MSE loss: 0.017639\n",
      "Iterative correction step 68/100, MSE loss: 0.017593\n",
      "Iterative correction step 69/100, MSE loss: 0.017548\n",
      "Iterative correction step 70/100, MSE loss: 0.017517\n",
      "Iterative correction step 71/100, MSE loss: 0.017471\n",
      "Iterative correction step 72/100, MSE loss: 0.017426\n",
      "Iterative correction step 73/100, MSE loss: 0.017395\n",
      "Iterative correction step 74/100, MSE loss: 0.017349\n",
      "Iterative correction step 75/100, MSE loss: 0.017319\n",
      "Iterative correction step 76/100, MSE loss: 0.017273\n",
      "Iterative correction step 77/100, MSE loss: 0.017242\n",
      "Iterative correction step 78/100, MSE loss: 0.017197\n",
      "Iterative correction step 79/100, MSE loss: 0.017166\n",
      "Iterative correction step 80/100, MSE loss: 0.017120\n",
      "Iterative correction step 81/100, MSE loss: 0.017090\n",
      "Iterative correction step 82/100, MSE loss: 0.017059\n",
      "Iterative correction step 83/100, MSE loss: 0.017014\n",
      "Iterative correction step 84/100, MSE loss: 0.016983\n",
      "Iterative correction step 85/100, MSE loss: 0.016953\n",
      "Iterative correction step 86/100, MSE loss: 0.016922\n",
      "Iterative correction step 87/100, MSE loss: 0.016876\n",
      "Iterative correction step 88/100, MSE loss: 0.016846\n",
      "Iterative correction step 89/100, MSE loss: 0.016815\n",
      "Iterative correction step 90/100, MSE loss: 0.016785\n",
      "Iterative correction step 91/100, MSE loss: 0.016754\n",
      "Iterative correction step 92/100, MSE loss: 0.016724\n",
      "Iterative correction step 93/100, MSE loss: 0.016693\n",
      "Iterative correction step 94/100, MSE loss: 0.016663\n",
      "Iterative correction step 95/100, MSE loss: 0.016632\n",
      "Iterative correction step 96/100, MSE loss: 0.016602\n",
      "Iterative correction step 97/100, MSE loss: 0.016571\n",
      "Iterative correction step 98/100, MSE loss: 0.016541\n",
      "Iterative correction step 99/100, MSE loss: 0.016510\n",
      "Iterative correction step 100/100, MSE loss: 0.016479\n",
      "Pruning + Correction complete.\n",
      "0 self_attn.v_proj\n",
      "Pruning ...\n",
      "torch.Size([2048, 5120]) torch.Size([5120, 5120])\n",
      "Iter 2/100, Loss=0.040039\n",
      "Iter 10/100, Loss=0.040039\n",
      "Iter 20/100, Loss=0.040039\n",
      "Iter 30/100, Loss=0.040039\n",
      "Iter 40/100, Loss=0.040039\n",
      "Iter 50/100, Loss=0.040039\n",
      "Iter 60/100, Loss=0.040039\n",
      "Iter 70/100, Loss=0.040039\n",
      "Iter 80/100, Loss=0.040039\n",
      "Iter 90/100, Loss=0.040039\n",
      "Iter 100/100, Loss=0.040009\n",
      "Starting iterative correction ...\n",
      "Iterative correction step 1/100, MSE loss: 0.015175\n",
      "Iterative correction step 2/100, MSE loss: 0.014801\n",
      "Iterative correction step 3/100, MSE loss: 0.014503\n",
      "Iterative correction step 4/100, MSE loss: 0.014267\n",
      "Iterative correction step 5/100, MSE loss: 0.014069\n",
      "Iterative correction step 6/100, MSE loss: 0.013893\n",
      "Iterative correction step 7/100, MSE loss: 0.013748\n",
      "Iterative correction step 8/100, MSE loss: 0.013618\n",
      "Iterative correction step 9/100, MSE loss: 0.013496\n",
      "Iterative correction step 10/100, MSE loss: 0.013390\n",
      "Iterative correction step 11/100, MSE loss: 0.013290\n",
      "Iterative correction step 12/100, MSE loss: 0.013199\n",
      "Iterative correction step 13/100, MSE loss: 0.013115\n",
      "Iterative correction step 14/100, MSE loss: 0.013039\n",
      "Iterative correction step 15/100, MSE loss: 0.012970\n",
      "Iterative correction step 16/100, MSE loss: 0.012894\n",
      "Iterative correction step 17/100, MSE loss: 0.012833\n",
      "Iterative correction step 18/100, MSE loss: 0.012772\n",
      "Iterative correction step 19/100, MSE loss: 0.012711\n",
      "Iterative correction step 20/100, MSE loss: 0.012657\n",
      "Iterative correction step 21/100, MSE loss: 0.012596\n",
      "Iterative correction step 22/100, MSE loss: 0.012550\n",
      "Iterative correction step 23/100, MSE loss: 0.012497\n",
      "Iterative correction step 24/100, MSE loss: 0.012451\n",
      "Iterative correction step 25/100, MSE loss: 0.012405\n",
      "Iterative correction step 26/100, MSE loss: 0.012360\n",
      "Iterative correction step 27/100, MSE loss: 0.012314\n",
      "Iterative correction step 28/100, MSE loss: 0.012276\n",
      "Iterative correction step 29/100, MSE loss: 0.012230\n",
      "Iterative correction step 30/100, MSE loss: 0.012192\n",
      "Iterative correction step 31/100, MSE loss: 0.012154\n",
      "Iterative correction step 32/100, MSE loss: 0.012115\n",
      "Iterative correction step 33/100, MSE loss: 0.012077\n",
      "Iterative correction step 34/100, MSE loss: 0.012047\n",
      "Iterative correction step 35/100, MSE loss: 0.012009\n",
      "Iterative correction step 36/100, MSE loss: 0.011978\n",
      "Iterative correction step 37/100, MSE loss: 0.011940\n",
      "Iterative correction step 38/100, MSE loss: 0.011909\n",
      "Iterative correction step 39/100, MSE loss: 0.011879\n",
      "Iterative correction step 40/100, MSE loss: 0.011848\n",
      "Iterative correction step 41/100, MSE loss: 0.011818\n",
      "Iterative correction step 42/100, MSE loss: 0.011787\n",
      "Iterative correction step 43/100, MSE loss: 0.011757\n",
      "Iterative correction step 44/100, MSE loss: 0.011734\n",
      "Iterative correction step 45/100, MSE loss: 0.011703\n",
      "Iterative correction step 46/100, MSE loss: 0.011673\n",
      "Iterative correction step 47/100, MSE loss: 0.011650\n",
      "Iterative correction step 48/100, MSE loss: 0.011620\n",
      "Iterative correction step 49/100, MSE loss: 0.011597\n",
      "Iterative correction step 50/100, MSE loss: 0.011574\n",
      "Iterative correction step 51/100, MSE loss: 0.011543\n",
      "Iterative correction step 52/100, MSE loss: 0.011520\n",
      "Iterative correction step 53/100, MSE loss: 0.011497\n",
      "Iterative correction step 54/100, MSE loss: 0.011475\n",
      "Iterative correction step 55/100, MSE loss: 0.011452\n",
      "Iterative correction step 56/100, MSE loss: 0.011429\n",
      "Iterative correction step 57/100, MSE loss: 0.011406\n",
      "Iterative correction step 58/100, MSE loss: 0.011383\n",
      "Iterative correction step 59/100, MSE loss: 0.011360\n",
      "Iterative correction step 60/100, MSE loss: 0.011337\n",
      "Iterative correction step 61/100, MSE loss: 0.011314\n",
      "Iterative correction step 62/100, MSE loss: 0.011299\n",
      "Iterative correction step 63/100, MSE loss: 0.011276\n",
      "Iterative correction step 64/100, MSE loss: 0.011253\n",
      "Iterative correction step 65/100, MSE loss: 0.011238\n",
      "Iterative correction step 66/100, MSE loss: 0.011215\n",
      "Iterative correction step 67/100, MSE loss: 0.011200\n",
      "Iterative correction step 68/100, MSE loss: 0.011177\n",
      "Iterative correction step 69/100, MSE loss: 0.011162\n",
      "Iterative correction step 70/100, MSE loss: 0.011139\n",
      "Iterative correction step 71/100, MSE loss: 0.011124\n",
      "Iterative correction step 72/100, MSE loss: 0.011101\n",
      "Iterative correction step 73/100, MSE loss: 0.011086\n",
      "Iterative correction step 74/100, MSE loss: 0.011070\n",
      "Iterative correction step 75/100, MSE loss: 0.011055\n",
      "Iterative correction step 76/100, MSE loss: 0.011032\n",
      "Iterative correction step 77/100, MSE loss: 0.011017\n",
      "Iterative correction step 78/100, MSE loss: 0.011002\n",
      "Iterative correction step 79/100, MSE loss: 0.010986\n",
      "Iterative correction step 80/100, MSE loss: 0.010971\n",
      "Iterative correction step 81/100, MSE loss: 0.010948\n",
      "Iterative correction step 82/100, MSE loss: 0.010933\n",
      "Iterative correction step 83/100, MSE loss: 0.010918\n",
      "Iterative correction step 84/100, MSE loss: 0.010902\n",
      "Iterative correction step 85/100, MSE loss: 0.010887\n",
      "Iterative correction step 86/100, MSE loss: 0.010872\n",
      "Iterative correction step 87/100, MSE loss: 0.010857\n",
      "Iterative correction step 88/100, MSE loss: 0.010841\n",
      "Iterative correction step 89/100, MSE loss: 0.010834\n",
      "Iterative correction step 90/100, MSE loss: 0.010818\n",
      "Iterative correction step 91/100, MSE loss: 0.010803\n",
      "Iterative correction step 92/100, MSE loss: 0.010788\n",
      "Iterative correction step 93/100, MSE loss: 0.010773\n",
      "Iterative correction step 94/100, MSE loss: 0.010757\n",
      "Iterative correction step 95/100, MSE loss: 0.010742\n",
      "Iterative correction step 96/100, MSE loss: 0.010735\n",
      "Iterative correction step 97/100, MSE loss: 0.010719\n",
      "Iterative correction step 98/100, MSE loss: 0.010704\n",
      "Iterative correction step 99/100, MSE loss: 0.010689\n",
      "Iterative correction step 100/100, MSE loss: 0.010681\n",
      "Pruning + Correction complete.\n",
      "0 self_attn.q_proj\n",
      "Pruning ...\n",
      "torch.Size([2048, 5120]) torch.Size([5120, 5120])\n",
      "Iter 2/100, Loss=0.134521\n",
      "Iter 10/100, Loss=0.134521\n",
      "Iter 20/100, Loss=0.134399\n",
      "Iter 30/100, Loss=0.134277\n",
      "Iter 40/100, Loss=0.134155\n",
      "Iter 50/100, Loss=0.134033\n",
      "Iter 60/100, Loss=0.133911\n",
      "Iter 70/100, Loss=0.133789\n",
      "Iter 80/100, Loss=0.133789\n",
      "Iter 90/100, Loss=0.133667\n",
      "Iter 100/100, Loss=0.133545\n",
      "Starting iterative correction ...\n",
      "Iterative correction step 1/100, MSE loss: 0.029526\n",
      "Iterative correction step 2/100, MSE loss: 0.027863\n",
      "Iterative correction step 3/100, MSE loss: 0.026596\n",
      "Iterative correction step 4/100, MSE loss: 0.025604\n",
      "Iterative correction step 5/100, MSE loss: 0.024826\n",
      "Iterative correction step 6/100, MSE loss: 0.024216\n",
      "Iterative correction step 7/100, MSE loss: 0.023727\n",
      "Iterative correction step 8/100, MSE loss: 0.023315\n",
      "Iterative correction step 9/100, MSE loss: 0.022980\n",
      "Iterative correction step 10/100, MSE loss: 0.022675\n",
      "Iterative correction step 11/100, MSE loss: 0.022430\n",
      "Iterative correction step 12/100, MSE loss: 0.022202\n",
      "Iterative correction step 13/100, MSE loss: 0.022003\n",
      "Iterative correction step 14/100, MSE loss: 0.021820\n",
      "Iterative correction step 15/100, MSE loss: 0.021652\n",
      "Iterative correction step 16/100, MSE loss: 0.021500\n",
      "Iterative correction step 17/100, MSE loss: 0.021362\n",
      "Iterative correction step 18/100, MSE loss: 0.021225\n",
      "Iterative correction step 19/100, MSE loss: 0.021103\n",
      "Iterative correction step 20/100, MSE loss: 0.020981\n",
      "Iterative correction step 21/100, MSE loss: 0.020859\n",
      "Iterative correction step 22/100, MSE loss: 0.020752\n",
      "Iterative correction step 23/100, MSE loss: 0.020645\n",
      "Iterative correction step 24/100, MSE loss: 0.020538\n",
      "Iterative correction step 25/100, MSE loss: 0.020447\n",
      "Iterative correction step 26/100, MSE loss: 0.020340\n",
      "Iterative correction step 27/100, MSE loss: 0.020248\n",
      "Iterative correction step 28/100, MSE loss: 0.020157\n",
      "Iterative correction step 29/100, MSE loss: 0.020081\n",
      "Iterative correction step 30/100, MSE loss: 0.019989\n",
      "Iterative correction step 31/100, MSE loss: 0.019913\n",
      "Iterative correction step 32/100, MSE loss: 0.019836\n",
      "Iterative correction step 33/100, MSE loss: 0.019760\n",
      "Iterative correction step 34/100, MSE loss: 0.019684\n",
      "Iterative correction step 35/100, MSE loss: 0.019608\n",
      "Iterative correction step 36/100, MSE loss: 0.019531\n",
      "Iterative correction step 37/100, MSE loss: 0.019455\n",
      "Iterative correction step 38/100, MSE loss: 0.019394\n",
      "Iterative correction step 39/100, MSE loss: 0.019333\n",
      "Iterative correction step 40/100, MSE loss: 0.019257\n",
      "Iterative correction step 41/100, MSE loss: 0.019196\n",
      "Iterative correction step 42/100, MSE loss: 0.019135\n",
      "Iterative correction step 43/100, MSE loss: 0.019073\n",
      "Iterative correction step 44/100, MSE loss: 0.019012\n",
      "Iterative correction step 45/100, MSE loss: 0.018951\n",
      "Iterative correction step 46/100, MSE loss: 0.018890\n",
      "Iterative correction step 47/100, MSE loss: 0.018829\n",
      "Iterative correction step 48/100, MSE loss: 0.018784\n",
      "Iterative correction step 49/100, MSE loss: 0.018723\n",
      "Iterative correction step 50/100, MSE loss: 0.018661\n",
      "Iterative correction step 51/100, MSE loss: 0.018616\n",
      "Iterative correction step 52/100, MSE loss: 0.018555\n",
      "Iterative correction step 53/100, MSE loss: 0.018509\n",
      "Iterative correction step 54/100, MSE loss: 0.018463\n",
      "Iterative correction step 55/100, MSE loss: 0.018402\n",
      "Iterative correction step 56/100, MSE loss: 0.018356\n",
      "Iterative correction step 57/100, MSE loss: 0.018311\n",
      "Iterative correction step 58/100, MSE loss: 0.018265\n",
      "Iterative correction step 59/100, MSE loss: 0.018219\n",
      "Iterative correction step 60/100, MSE loss: 0.018158\n",
      "Iterative correction step 61/100, MSE loss: 0.018112\n",
      "Iterative correction step 62/100, MSE loss: 0.018066\n",
      "Iterative correction step 63/100, MSE loss: 0.018036\n",
      "Iterative correction step 64/100, MSE loss: 0.017990\n",
      "Iterative correction step 65/100, MSE loss: 0.017944\n",
      "Iterative correction step 66/100, MSE loss: 0.017899\n",
      "Iterative correction step 67/100, MSE loss: 0.017853\n",
      "Iterative correction step 68/100, MSE loss: 0.017807\n",
      "Iterative correction step 69/100, MSE loss: 0.017776\n",
      "Iterative correction step 70/100, MSE loss: 0.017731\n",
      "Iterative correction step 71/100, MSE loss: 0.017685\n",
      "Iterative correction step 72/100, MSE loss: 0.017654\n",
      "Iterative correction step 73/100, MSE loss: 0.017609\n",
      "Iterative correction step 74/100, MSE loss: 0.017578\n",
      "Iterative correction step 75/100, MSE loss: 0.017532\n",
      "Iterative correction step 76/100, MSE loss: 0.017502\n",
      "Iterative correction step 77/100, MSE loss: 0.017456\n",
      "Iterative correction step 78/100, MSE loss: 0.017426\n",
      "Iterative correction step 79/100, MSE loss: 0.017395\n",
      "Iterative correction step 80/100, MSE loss: 0.017349\n",
      "Iterative correction step 81/100, MSE loss: 0.017319\n",
      "Iterative correction step 82/100, MSE loss: 0.017288\n",
      "Iterative correction step 83/100, MSE loss: 0.017242\n",
      "Iterative correction step 84/100, MSE loss: 0.017212\n",
      "Iterative correction step 85/100, MSE loss: 0.017181\n",
      "Iterative correction step 86/100, MSE loss: 0.017151\n",
      "Iterative correction step 87/100, MSE loss: 0.017105\n",
      "Iterative correction step 88/100, MSE loss: 0.017075\n",
      "Iterative correction step 89/100, MSE loss: 0.017044\n",
      "Iterative correction step 90/100, MSE loss: 0.017014\n",
      "Iterative correction step 91/100, MSE loss: 0.016983\n",
      "Iterative correction step 92/100, MSE loss: 0.016953\n",
      "Iterative correction step 93/100, MSE loss: 0.016922\n",
      "Iterative correction step 94/100, MSE loss: 0.016891\n",
      "Iterative correction step 95/100, MSE loss: 0.016861\n",
      "Iterative correction step 96/100, MSE loss: 0.016830\n",
      "Iterative correction step 97/100, MSE loss: 0.016800\n",
      "Iterative correction step 98/100, MSE loss: 0.016769\n",
      "Iterative correction step 99/100, MSE loss: 0.016739\n",
      "Iterative correction step 100/100, MSE loss: 0.016708\n",
      "Pruning + Correction complete.\n",
      "0 self_attn.out_proj\n",
      "Pruning ...\n",
      "torch.Size([2048, 5120]) torch.Size([5120, 5120])\n",
      "Iter 2/100, Loss=0.024780\n",
      "Iter 10/100, Loss=0.024780\n",
      "Iter 20/100, Loss=0.024780\n",
      "Iter 30/100, Loss=0.024780\n",
      "Iter 40/100, Loss=0.024765\n",
      "Iter 50/100, Loss=0.024765\n",
      "Iter 60/100, Loss=0.024765\n",
      "Iter 70/100, Loss=0.024750\n",
      "Iter 80/100, Loss=0.024750\n",
      "Iter 90/100, Loss=0.024734\n",
      "Iter 100/100, Loss=0.024734\n",
      "Starting iterative correction ...\n",
      "Iterative correction step 1/100, MSE loss: 0.000095\n",
      "Iterative correction step 2/100, MSE loss: 0.000095\n",
      "Iterative correction step 3/100, MSE loss: 0.000095\n",
      "Iterative correction step 4/100, MSE loss: 0.000095\n",
      "Iterative correction step 5/100, MSE loss: 0.000095\n",
      "Iterative correction step 6/100, MSE loss: 0.000095\n",
      "Iterative correction step 7/100, MSE loss: 0.000095\n",
      "Iterative correction step 8/100, MSE loss: 0.000095\n",
      "Iterative correction step 9/100, MSE loss: 0.000095\n",
      "Iterative correction step 10/100, MSE loss: 0.000095\n",
      "Iterative correction step 11/100, MSE loss: 0.000095\n",
      "Iterative correction step 12/100, MSE loss: 0.000095\n",
      "Iterative correction step 13/100, MSE loss: 0.000095\n",
      "Iterative correction step 14/100, MSE loss: 0.000095\n",
      "Iterative correction step 15/100, MSE loss: 0.000095\n",
      "Iterative correction step 16/100, MSE loss: 0.000095\n",
      "Iterative correction step 17/100, MSE loss: 0.000095\n",
      "Iterative correction step 18/100, MSE loss: 0.000095\n",
      "Iterative correction step 19/100, MSE loss: 0.000095\n",
      "Iterative correction step 20/100, MSE loss: 0.000095\n",
      "Iterative correction step 21/100, MSE loss: 0.000095\n",
      "Iterative correction step 22/100, MSE loss: 0.000095\n",
      "Iterative correction step 23/100, MSE loss: 0.000095\n",
      "Iterative correction step 24/100, MSE loss: 0.000095\n",
      "Iterative correction step 25/100, MSE loss: 0.000095\n",
      "Iterative correction step 26/100, MSE loss: 0.000095\n",
      "Iterative correction step 27/100, MSE loss: 0.000095\n",
      "Iterative correction step 28/100, MSE loss: 0.000095\n",
      "Iterative correction step 29/100, MSE loss: 0.000095\n",
      "Iterative correction step 30/100, MSE loss: 0.000095\n",
      "Iterative correction step 31/100, MSE loss: 0.000095\n",
      "Iterative correction step 32/100, MSE loss: 0.000095\n",
      "Iterative correction step 33/100, MSE loss: 0.000095\n",
      "Iterative correction step 34/100, MSE loss: 0.000095\n",
      "Iterative correction step 35/100, MSE loss: 0.000095\n",
      "Iterative correction step 36/100, MSE loss: 0.000095\n",
      "Iterative correction step 37/100, MSE loss: 0.000095\n",
      "Iterative correction step 38/100, MSE loss: 0.000095\n",
      "Iterative correction step 39/100, MSE loss: 0.000095\n",
      "Iterative correction step 40/100, MSE loss: 0.000095\n",
      "Iterative correction step 41/100, MSE loss: 0.000095\n",
      "Iterative correction step 42/100, MSE loss: 0.000095\n",
      "Iterative correction step 43/100, MSE loss: 0.000094\n",
      "Iterative correction step 44/100, MSE loss: 0.000094\n",
      "Iterative correction step 45/100, MSE loss: 0.000094\n",
      "Iterative correction step 46/100, MSE loss: 0.000094\n",
      "Iterative correction step 47/100, MSE loss: 0.000094\n",
      "Iterative correction step 48/100, MSE loss: 0.000094\n",
      "Iterative correction step 49/100, MSE loss: 0.000094\n",
      "Iterative correction step 50/100, MSE loss: 0.000094\n",
      "Iterative correction step 51/100, MSE loss: 0.000094\n",
      "Iterative correction step 52/100, MSE loss: 0.000094\n",
      "Iterative correction step 53/100, MSE loss: 0.000094\n",
      "Iterative correction step 54/100, MSE loss: 0.000094\n",
      "Iterative correction step 55/100, MSE loss: 0.000094\n",
      "Iterative correction step 56/100, MSE loss: 0.000094\n",
      "Iterative correction step 57/100, MSE loss: 0.000094\n",
      "Iterative correction step 58/100, MSE loss: 0.000094\n",
      "Iterative correction step 59/100, MSE loss: 0.000094\n",
      "Iterative correction step 60/100, MSE loss: 0.000094\n",
      "Iterative correction step 61/100, MSE loss: 0.000094\n",
      "Iterative correction step 62/100, MSE loss: 0.000094\n",
      "Iterative correction step 63/100, MSE loss: 0.000094\n",
      "Iterative correction step 64/100, MSE loss: 0.000094\n",
      "Iterative correction step 65/100, MSE loss: 0.000094\n",
      "Iterative correction step 66/100, MSE loss: 0.000094\n",
      "Iterative correction step 67/100, MSE loss: 0.000094\n",
      "Iterative correction step 68/100, MSE loss: 0.000094\n",
      "Iterative correction step 69/100, MSE loss: 0.000094\n",
      "Iterative correction step 70/100, MSE loss: 0.000094\n",
      "Iterative correction step 71/100, MSE loss: 0.000094\n",
      "Iterative correction step 72/100, MSE loss: 0.000094\n",
      "Iterative correction step 73/100, MSE loss: 0.000094\n",
      "Iterative correction step 74/100, MSE loss: 0.000094\n",
      "Iterative correction step 75/100, MSE loss: 0.000094\n",
      "Iterative correction step 76/100, MSE loss: 0.000094\n",
      "Iterative correction step 77/100, MSE loss: 0.000094\n",
      "Iterative correction step 78/100, MSE loss: 0.000094\n",
      "Iterative correction step 79/100, MSE loss: 0.000094\n",
      "Iterative correction step 80/100, MSE loss: 0.000094\n",
      "Iterative correction step 81/100, MSE loss: 0.000094\n",
      "Iterative correction step 82/100, MSE loss: 0.000094\n",
      "Iterative correction step 83/100, MSE loss: 0.000094\n",
      "Iterative correction step 84/100, MSE loss: 0.000094\n",
      "Iterative correction step 85/100, MSE loss: 0.000094\n",
      "Iterative correction step 86/100, MSE loss: 0.000094\n",
      "Iterative correction step 87/100, MSE loss: 0.000094\n",
      "Iterative correction step 88/100, MSE loss: 0.000094\n",
      "Iterative correction step 89/100, MSE loss: 0.000094\n",
      "Iterative correction step 90/100, MSE loss: 0.000094\n",
      "Iterative correction step 91/100, MSE loss: 0.000094\n",
      "Iterative correction step 92/100, MSE loss: 0.000094\n",
      "Iterative correction step 93/100, MSE loss: 0.000094\n",
      "Iterative correction step 94/100, MSE loss: 0.000094\n",
      "Iterative correction step 95/100, MSE loss: 0.000094\n",
      "Iterative correction step 96/100, MSE loss: 0.000094\n",
      "Iterative correction step 97/100, MSE loss: 0.000094\n",
      "Iterative correction step 98/100, MSE loss: 0.000094\n",
      "Iterative correction step 99/100, MSE loss: 0.000094\n",
      "Iterative correction step 100/100, MSE loss: 0.000094\n",
      "Pruning + Correction complete.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.00 GiB. GPU 1 has a total capacity of 22.18 GiB of which 1.39 GiB is free. Including non-PyTorch memory, this process has 20.79 GiB memory in use. Of the allocated memory 17.65 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/sxy/sparseLLM repo/SparseLLM/opt_main.py:46\u001b[0m\n\u001b[1;32m     43\u001b[0m         model\u001b[38;5;241m.\u001b[39msave_pretrained(args\u001b[38;5;241m.\u001b[39msave)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sxy/sparseLLM repo/SparseLLM/opt_main.py:36\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m dataloader, testloader \u001b[38;5;241m=\u001b[39m get_loaders(args\u001b[38;5;241m.\u001b[39mdataset, nsamples\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnsamples, seed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed, model\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel, seqlen\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mseqlen)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (args\u001b[38;5;241m.\u001b[39msparsity \u001b[38;5;129;01mor\u001b[39;00m args\u001b[38;5;241m.\u001b[39mprunen) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mgmp:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mopt_sparsellm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudan\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwikitext2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc4\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     39\u001b[0m     dataloader, testloader \u001b[38;5;241m=\u001b[39m get_loaders(dataset, seed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed, model\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel, seqlen\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mseqlen)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sxy/sparseLLM repo/SparseLLM/model_utils.py:147\u001b[0m, in \u001b[0;36mopt_sparsellm\u001b[0;34m(model, dataloader, dev, args)\u001b[0m\n\u001b[1;32m    144\u001b[0m gpts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_out\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    146\u001b[0m hidden_z_list \u001b[38;5;241m=\u001b[39m gpts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_out\n\u001b[0;32m--> 147\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_z_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m hidden_z_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m gpts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_out\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 1 has a total capacity of 22.18 GiB of which 1.39 GiB is free. Including non-PyTorch memory, this process has 20.79 GiB memory in use. Of the allocated memory 17.65 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "%run opt_main.py --model 'facebook/opt-13b' --sparsity 0.1 --cudan \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
